{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pruebascomparativas.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "XfFahBIOYoro",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Análisis de Sentimientos a Nivel de Texto: Pruebas comparativas\n",
        "---\n",
        "\n",
        "> **Proyecto final de Asignatura Sistemas Computacionales <br>\n",
        "Escuela de Ingeniería de Sistemas <br>\n",
        "Universidad de Los Andes <br>\n",
        "Autor: Jhonathan Abreu <br>**\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "La función del presente notebook es generar estadísticas que permitan decidir el mejor vectorizador y clasificador a utilizar.\n",
        "\n",
        "<br>\n",
        "\n",
        "## Módulos necesarios\n",
        "---\n",
        "\n",
        "Los módulos necesarios son los siguientes:\n",
        "\n",
        "*   **gensim**: para el modelo Word2Vec.\n",
        "*   **tqdm**: una barra de progreso.\n",
        "*   **unidecode**: para la eliminacion de acentos.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "sp13JEgaDztq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "697d888e-1d49-4818-9761-9a9c51265c48",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529527317983,
          "user_tz": 240,
          "elapsed": 9957,
          "user": {
            "displayName": "Jhonathan Abreu",
            "photoUrl": "//lh4.googleusercontent.com/-4bMUXodd-B0/AAAAAAAAAAI/AAAAAAAAAlY/Lc07exuN7Aw/s50-c-k-no/photo.jpg",
            "userId": "117165292541590122752"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim  # For the Word2Vec model\n",
        "!pip install tqdm    # Just for using a progress bar\n",
        "!pip install bokeh   # For graphs\n",
        "!pip install unidecode"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.5.7)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.7.41)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.48.0)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.4.16)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
            "Requirement already satisfied: botocore<1.11.0,>=1.10.41 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.10.41)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.41->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.41->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.23.4)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (0.13.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh) (17.1)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (1.14.5)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from bokeh) (1.11.0)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh) (4.5.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh) (3.12)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.5.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=16.8->bokeh) (2.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh) (1.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.0.22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "54Ff9OqsZVjW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Subida del dataset\n",
        "---\n",
        "\n",
        "Si está utilizando Google Colaboratory, suba el dataset con el siguiente código. En caso contrario, escriba la ruta absoluta del archivo en el bloque de código de ingesta."
      ]
    },
    {
      "metadata": {
        "id": "-f5QjKKr_Mm7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "70424e19-aee8-466f-b59b-263e9f19efb5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529526936283,
          "user_tz": 240,
          "elapsed": 8144,
          "user": {
            "displayName": "Jhonathan Abreu",
            "photoUrl": "//lh4.googleusercontent.com/-4bMUXodd-B0/AAAAAAAAAAI/AAAAAAAAAlY/Lc07exuN7Aw/s50-c-k-no/photo.jpg",
            "userId": "117165292541590122752"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-33cae5b4-60e3-49b7-8d27-edbb3ba98723\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-33cae5b4-60e3-49b7-8d27-edbb3ba98723\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving labeled_data.csv to labeled_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G-BuxlexaKC6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Carga delibrerías y módulos\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "jTRY8cO3_b54",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ab502ffe-b3b5-4d0a-de22-eef77735cae7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529527320128,
          "user_tz": 240,
          "elapsed": 2123,
          "user": {
            "displayName": "Jhonathan Abreu",
            "photoUrl": "//lh4.googleusercontent.com/-4bMUXodd-B0/AAAAAAAAAAI/AAAAAAAAAlY/Lc07exuN7Aw/s50-c-k-no/photo.jpg",
            "userId": "117165292541590122752"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from string import punctuation\n",
        "from random import shuffle\n",
        "import io\n",
        "import csv\n",
        "\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Descarga y carga de la lista de palabras vacías de NLTK\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.data import load\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn import svm\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "from unidecode import unidecode\n",
        "\n",
        "import time\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "TFBIf33Yas6z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ingesta del dataset\n",
        "---\n",
        "\n",
        "Con este código, cargamos los datos del archivo."
      ]
    },
    {
      "metadata": {
        "id": "xddQRc-9AL0O",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "f9dadf62-5af7-4347-fd8e-479747e621ba",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529527322645,
          "user_tz": 240,
          "elapsed": 634,
          "user": {
            "displayName": "Jhonathan Abreu",
            "photoUrl": "//lh4.googleusercontent.com/-4bMUXodd-B0/AAAAAAAAAAI/AAAAAAAAAlY/Lc07exuN7Aw/s50-c-k-no/photo.jpg",
            "userId": "117165292541590122752"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def ingest(datasetFileName):\n",
        "    data = pd.read_csv(datasetFileName, header = None)\n",
        "    data.columns = ['sentences', 'sentiment']\n",
        "    data['sentiment'] = data['sentiment'].map({\n",
        "                                                'positivo': 2,\n",
        "                                                'neutral': 1,\n",
        "                                                'negativo': 0\n",
        "                                              })\n",
        "    data.reset_index(inplace = True)\n",
        "    data.drop('index', axis = 1, inplace = True)\n",
        "    print('dataset loaded with shape', data.shape)\n",
        "    return data\n",
        "\n",
        "datasetFileName = 'labeled_data.csv'\n",
        "data = ingest(datasetFileName)\n",
        "display(data.head())\n",
        "\n",
        "trainSentences, testSentences, trainLabels, testLabels = \\\n",
        "    train_test_split(np.array(data.sentences),\n",
        "                     np.array(data.sentiment), test_size = 0.2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset loaded with shape (600, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>La canción que acaba de sacar Sia es muy buena, suena muy bien</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>La música de Maluma es realmente mala y sin sentido, no se como es que a las persona les gusta</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hoy es un día normal y tranquilo</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Que bello día</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Estoy enojado contigo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                        sentences  \\\n",
              "0  La canción que acaba de sacar Sia es muy buena, suena muy bien                                   \n",
              "1  La música de Maluma es realmente mala y sin sentido, no se como es que a las persona les gusta   \n",
              "2  Hoy es un día normal y tranquilo                                                                 \n",
              "3  Que bello día                                                                                    \n",
              "4  Estoy enojado contigo                                                                            \n",
              "\n",
              "   sentiment  \n",
              "0  2          \n",
              "1  0          \n",
              "2  1          \n",
              "3  2          \n",
              "4  0          "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XcJzhAaiat9a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pruebas\n",
        "---\n",
        "\n",
        "Se van a probar diferentes SVM y Red Neuronal Convolucional con dos tipos de vectorizadores, Word2Vec y CountVectorizer (Bag-of-Words, propuesto en el artículo)."
      ]
    },
    {
      "metadata": {
        "id": "9f1LTCxKAnwS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "06cefa51-547f-4f83-f3eb-d114d9f281d4"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "## FUNCIONES DE LIMPIEZA DEL DATASET ###########################################\n",
        "################################################################################\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "spanishStopWords = stopwords.words('spanish')\n",
        "\n",
        "# Carga y extensión de la lista de signos de puntuación y otros símbolos.\n",
        "from string import punctuation\n",
        "\n",
        "nonWords = list(punctuation)\n",
        "nonWords.extend(['¿', '¡'])  # Se agregan estos símbolos (español)\n",
        "nonWords.extend(map(str,range(10)))  # Se agregan los dígitos numéricos\n",
        "\n",
        "# Stemmer, objeto que llevará las palabras a sus raíces \n",
        "stemmer = SnowballStemmer('spanish')\n",
        "\n",
        "# Función que aplica el stemming\n",
        "def stem_tokens(tokens, stemmer):\n",
        "    stemmedTokens = []\n",
        "    for token in tokens:\n",
        "        stemmedTokens.append(stemmer.stem(token))\n",
        "        \n",
        "    return stemmedTokens\n",
        "\n",
        "# Función que limpia y tokeniza las frases\n",
        "def tokenize(text):\n",
        "    # Eliminación de símbolos y números\n",
        "    text = ''.join([c for c in text if c not in nonWords])\n",
        "    # Tokeninazión\n",
        "    tokens =  word_tokenize(text)\n",
        "\n",
        "    # Stemming\n",
        "    try:\n",
        "        stemmedTokens = stem_tokens(tokens, stemmer)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(text)\n",
        "        stemmedTokens = ['']\n",
        "        \n",
        "    return stemmedTokens\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "## PRUEBAS CON WORD2VEC ########################################################\n",
        "################################################################################\n",
        "\n",
        "comparisonResults = None\n",
        "\n",
        "# Word2Vec\n",
        "\n",
        "def buildSentenceVector(tokens, size, wordsModel):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0\n",
        "        \n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec += wordsModel[word].reshape((1, size)) * tfidf[word]\n",
        "            count += 1.\n",
        "        except KeyError: # handling the case where the token is not\n",
        "                         # in the corpus. useful for testing.\n",
        "            continue\n",
        "    if count != 0:\n",
        "        vec /= count\n",
        "    return vec\n",
        "\n",
        "def compareSVMClassifiers(classifiers, xTrain, yTrain, xTest, yTest,\n",
        "                          verbose = False):\n",
        "    nClassifiers = len(classifiers.keys())\n",
        "    culumnNames = ['Clasificador', 'Exactitud Entrenamiento',\n",
        "                   'Exactitud Prueba']\n",
        "    results = pd.DataFrame(data = np.zeros(shape = (nClassifiers, 3)),\n",
        "                           columns = culumnNames)\n",
        "    counter = 0\n",
        "    for key, classifier in classifiers.items():\n",
        "        tic = time.clock()\n",
        "        classifier.fit(xTrain, yTrain)\n",
        "        toc = time.clock()\n",
        "        elapsedTime = toc - tic\n",
        "        trainAccuracy = classifier.score(xTrain, yTrain)\n",
        "        testAccuracy = classifier.score(xTest, yTest)\n",
        "        results.loc[counter, 'Clasificador'] = key\n",
        "        results.loc[counter, 'Exactitud Entrenamiento'] = trainAccuracy\n",
        "        results.loc[counter, 'Exactitud Prueba'] = testAccuracy\n",
        "        if verbose:\n",
        "            print(\"{c} entrenado en {f:.2f} s\".format(c = key, f = elapsedTime))\n",
        "        counter += 1\n",
        "        \n",
        "    return results\n",
        "\n",
        "print(('\\n\\n*****************************************************************\\n'\n",
        "       'INICIANDO PRUEBAS CON WORD2VEC\\n'\n",
        "       '*****************************************************************\\n'\n",
        "       '\\n'))\n",
        "\n",
        "vectorDimensions = [200, 500, 800, 1000]\n",
        "\n",
        "for vectorDimension in vectorDimensions:\n",
        "    print(('\\t***************************************************************\\n'\n",
        "           '\\tW2V {}\\n'\n",
        "           '\\t***************************************************************\\n'\n",
        "           '\\t\\n').format(vectorDimension))\n",
        "    \n",
        "    svmsWithW2V = {}\n",
        "    \n",
        "    trainSentencesTokens = [sentence.lower() for sentence in trainSentences]\n",
        "    trainSentencesTokens = [unidecode(sentence)\n",
        "                            for sentence in trainSentencesTokens]\n",
        "    trainSentencesTokens = [tokenize(sentence)\n",
        "                            for sentence in trainSentencesTokens]\n",
        "    \n",
        "    testSentencesTokens = [sentence.lower() for sentence in testSentences]\n",
        "    testSentencesTokens = [unidecode(sentence)\n",
        "                            for sentence in testSentencesTokens]\n",
        "    testSentencesTokens = [tokenize(sentence)\n",
        "                            for sentence in testSentencesTokens]\n",
        "    \n",
        "    wordsModel = Word2Vec(trainSentencesTokens, size = vectorDimension,\n",
        "                          min_count = 3, window = 5)\n",
        "    wordsModel.train([train for train in tqdm(trainSentencesTokens)],\n",
        "                     total_examples = len(trainSentencesTokens), epochs = 15)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(analyzer = lambda x: x)\n",
        "    matrix = vectorizer.fit_transform([sentence\n",
        "                                       for sentence in tqdm(trainSentencesTokens)])\n",
        "    tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
        "    \n",
        "    trainSentencesVectors = np.concatenate([buildSentenceVector(w, vectorDimension,\n",
        "                                                                wordsModel)\n",
        "                                            for w in tqdm(trainSentencesTokens)])\n",
        "    trainSentencesVectors = scale(trainSentencesVectors)\n",
        "\n",
        "    testSentencesVectors = np.concatenate([buildSentenceVector(w, vectorDimension,\n",
        "                                                               wordsModel)\n",
        "                                           for w in tqdm(testSentencesTokens)])\n",
        "    testSentencesVectors = scale(testSentencesVectors)\n",
        "\n",
        "    # SVM Lineal\n",
        "    print(('\\n\\t\\t*************************************************************\\n'\n",
        "           '\\t\\tSVM Lineal\\n'\n",
        "           '\\t\\t*************************************************************\\n'\n",
        "           '\\n'))\n",
        "    parameters = [\n",
        "        {\n",
        "            'kernel': ['linear'],\n",
        "            'C': [0.01, 0.1, 1, 10, 100]\n",
        "        }\n",
        "    ]\n",
        "    optimalLinearSVM = GridSearchCV(svm.SVC(decision_function_shape = 'ovr'),\n",
        "                                parameters, cv = 3, n_jobs = -1, verbose = 0)\n",
        "    optimalLinearSVM.fit(trainSentencesVectors, trainLabels)\n",
        "    \n",
        "    # SVM Radial\n",
        "    print(('\\t\\t*************************************************************\\n'\n",
        "           '\\t\\tSVM Radial\\n'\n",
        "           '\\t\\t*************************************************************\\n'\n",
        "           '\\n'))\n",
        "    parameters = [\n",
        "        {\n",
        "            'kernel': ['rbf'],\n",
        "            'gamma': [1e-4, 1e-3, 1e-2, 1e-1],\n",
        "            'C': [0.01, 0.1, 1, 10, 100]\n",
        "        }\n",
        "    ]\n",
        "    optimalRadialSVM = GridSearchCV(svm.SVC(decision_function_shape = 'ovr'),\n",
        "                                parameters, cv = 3, n_jobs = -1, verbose = 0)\n",
        "    optimalRadialSVM.fit(trainSentencesVectors, trainLabels)\n",
        "    \n",
        "    # SVM Polinomico\n",
        "    print(('\\t\\t*************************************************************\\n'\n",
        "           '\\t\\tSVM Polinómico\\n'\n",
        "           '\\t\\t*************************************************************\\n'\n",
        "           '\\n'))\n",
        "    parameters = [\n",
        "        {\n",
        "            'kernel': ['poly'],\n",
        "            'gamma': [1e-4, 1e-3, 1e-2, 1e-1],\n",
        "            'C': [0.01, 0.1, 1, 10, 100],\n",
        "            'degree': [2, 3]\n",
        "        }\n",
        "    ]\n",
        "    optimalPolySVM = GridSearchCV(svm.SVC(decision_function_shape = 'ovr'),\n",
        "                              parameters, cv = 3, n_jobs = -1, verbose = 0)\n",
        "    optimalPolySVM.fit(trainSentencesVectors, trainLabels)\n",
        "    \n",
        "    # Comparacion\n",
        "    svmsWithW2V['SVM Lineal con W2V {}'.format(vectorDimension)] = \\\n",
        "        svm.SVC(kernel = optimalLinearSVM.best_params_['kernel'],\n",
        "                C = optimalLinearSVM.best_params_['C'])\n",
        "        \n",
        "    svmsWithW2V['SVM Radial con W2V {}'.format(vectorDimension)] = \\\n",
        "        svm.SVC(kernel = optimalRadialSVM.best_params_['kernel'],\n",
        "                C = optimalRadialSVM.best_params_['C'],\n",
        "                gamma = optimalRadialSVM.best_params_['gamma'])\n",
        "        \n",
        "    svmsWithW2V['SVM Polinómico con W2V {}'.format(vectorDimension)] = \\\n",
        "        svm.SVC(kernel = optimalPolySVM.best_params_['kernel'],\n",
        "                C = optimalPolySVM.best_params_['C'],\n",
        "                gamma = optimalPolySVM.best_params_['gamma'],\n",
        "                degree = optimalPolySVM.best_params_['degree'])\n",
        "    \n",
        "    \n",
        "    results = compareSVMClassifiers(svmsWithW2V,\n",
        "                                    trainSentencesVectors, trainLabels,\n",
        "                                    testSentencesVectors, testLabels)\n",
        "    if comparisonResults is None:\n",
        "        comparisonResults = results.copy()\n",
        "    else:\n",
        "        comparisonResults = comparisonResults.append(results)\n",
        "        \n",
        "    \n",
        "    # Redes neuronales convolucionales\n",
        "    \n",
        "    print(('\\t\\t*************************************************************\\n'\n",
        "           '\\t\\tRNC\\n'\n",
        "           '\\t\\t*************************************************************\\n'\n",
        "           '\\n'))\n",
        "    \n",
        "    # Redimensionando los vectores para ser alimentados a la RNC\n",
        "    trainVectors = trainSentencesVectors.reshape((trainSentencesVectors.shape[0], 1,\n",
        "                                         trainSentencesVectors.shape[1]))\n",
        "    testVectors = testSentencesVectors.reshape((testSentencesVectors.shape[0], 1,\n",
        "                                       testSentencesVectors.shape[1]))\n",
        "    \n",
        "    # Conversión de las etiquetas (One-Hot)\n",
        "    trainLabelsCat = keras.utils.np_utils.to_categorical(trainLabels, 3)\n",
        "    testLabelsCat = keras.utils.np_utils.to_categorical(testLabels, 3)\n",
        "    \n",
        "    def buildModel(inputDimension):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv1D(128, 1, input_shape = (1, inputDimension),\n",
        "                         activation = 'relu'))\n",
        "        model.add(Conv1D(32, 1, activation = 'relu'))\n",
        "\n",
        "        model.add(Flatten())\n",
        "\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Dense(64, activation = 'relu'))\n",
        "        model.add(Dense(32, activation = 'relu'))    \n",
        "        model.add(Dense(16, activation = 'relu'))\n",
        "        model.add(Dense(3, activation = 'sigmoid'))\n",
        "\n",
        "        model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy',\n",
        "                      metrics = ['accuracy'])\n",
        "\n",
        "        return model\n",
        "    \n",
        "    modelFileName = 'modelo.hdf5'\n",
        "    \n",
        "    # Checkpoint (número 1 en la lista anterior)\n",
        "    checkpointer = ModelCheckpoint(filepath = modelFileName, \n",
        "                                   monitor = 'val_acc', verbose = 0,\n",
        "                                   save_best_only = True, mode = 'auto')\n",
        "\n",
        "    # Reducción de taza de aprendizaje (número 2 de la lista anterior)\n",
        "    learningRateReducer = ReduceLROnPlateau(monitor = 'val_acc', factor = 0.2,\n",
        "                                            patience = 5, min_lr = 0.001,\n",
        "                                            verbose = 0)\n",
        "\n",
        "    # Detener el entrenamiento al alcanzar el \"óptimo\"\n",
        "    # (número 3 de la lista anterior)\n",
        "    earlyStop = EarlyStopping(monitor = 'val_acc', patience = 50, verbose = 0,\n",
        "                              mode = 'auto')\n",
        "    \n",
        "    model = buildModel(trainVectors.shape[2])\n",
        "    model.fit(trainVectors, trainLabelsCat, batch_size = 32,\n",
        "               validation_data = (testVectors, testLabelsCat), epochs = 1000,\n",
        "               callbacks = [checkpointer, learningRateReducer, earlyStop],\n",
        "               verbose = 0, shuffle = False)\n",
        "    \n",
        "    # Contrucción del modelo\n",
        "    model = buildModel(trainVectors.shape[2])\n",
        "\n",
        "    # Carga delos pesos\n",
        "    model.load_weights(modelFileName)\n",
        "    acc = model.evaluate(trainVectors, trainLabelsCat, verbose = 0)[1]\n",
        "    valacc = model.evaluate(testVectors, testLabelsCat, verbose = 0)[1]\n",
        "    culumnNames = ['Clasificador', 'Exactitud Entrenamiento',\n",
        "                   'Exactitud Prueba']\n",
        "    results = pd.DataFrame(data = np.zeros(shape = (1, 3)),\n",
        "                           columns = culumnNames)\n",
        "    results.loc[0, 'Clasificador'] = 'RNC con W2V {}'.format(vectorDimension)\n",
        "    results.loc[0, 'Exactitud Entrenamiento'] = acc\n",
        "    results.loc[0, 'Exactitud Prueba'] = valacc\n",
        "        \n",
        "    comparisonResults = comparisonResults.append(results)\n",
        "    \n",
        "    \n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "## PRUEBAS CON COUNTVECTORIZER #################################################\n",
        "################################################################################\n",
        "\n",
        "print(('\\n\\n*****************************************************************\\n'\n",
        "       'INICIANDO PRUEBAS CON CountVectorizer\\n'\n",
        "       '\\n\\n*****************************************************************\\n'\n",
        "       '\\n'))\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer = 'word', tokenizer = tokenize,\n",
        "                             lowercase = True, strip_accents = 'unicode',\n",
        "                             stop_words = spanishStopWords, ngram_range = (1,2))\n",
        "\n",
        "# Entrenamiento\n",
        "vectorizer.fit(trainSentences)\n",
        "\n",
        "# Oraciones a vectores\n",
        "trainVectors = vectorizer.transform(trainSentences).toarray()\n",
        "testVectors = vectorizer.transform(testSentences).toarray()\n",
        "\n",
        "vectorDimension = trainVectors.shape[1]\n",
        "\n",
        "# SVM Lineal\n",
        "\n",
        "print(('\\n\\t***************************************************************\\n'\n",
        "       '\\tSVM Lineal\\n'\n",
        "       '\\t***************************************************************\\n'\n",
        "       '\\n'))\n",
        "parameters = [\n",
        "    {\n",
        "        'kernel': ['linear'],\n",
        "        'C': [0.01, 0.1, 1, 10, 100]\n",
        "    }\n",
        "]\n",
        "optimalLinearSVM = GridSearchCV(svm.SVC(decision_function_shape = 'ovr'),\n",
        "                                parameters, cv = 3, n_jobs = -1, verbose = 0)\n",
        "optimalLinearSVM.fit(trainSentencesVectors, trainLabels)\n",
        "    \n",
        "# SVM Radial\n",
        "\n",
        "print(('\\t***************************************************************\\n'\n",
        "       '\\tSVM Radial\\n'\n",
        "       '\\t***************************************************************\\n'\n",
        "       '\\n'))\n",
        "parameters = [\n",
        "    {\n",
        "        'kernel': ['rbf'],\n",
        "        'gamma': [1e-4, 1e-3, 1e-2, 1e-1],\n",
        "        'C': [0.01, 0.1, 1, 10, 100]\n",
        "    }\n",
        "]\n",
        "optimalRadialSVM = GridSearchCV(svm.SVC(decision_function_shape = 'ovr'),\n",
        "                                parameters, cv = 3, n_jobs = -1, verbose = 0)\n",
        "optimalRadialSVM.fit(trainSentencesVectors, trainLabels)\n",
        "    \n",
        "# SVM Polinomico\n",
        "print(('\\t***************************************************************\\n'\n",
        "       '\\tSVM Polinómico\\n'\n",
        "       '\\t***************************************************************\\n'\n",
        "       '\\n'))\n",
        "parameters = [\n",
        "    {\n",
        "        'kernel': ['poly'],\n",
        "        'gamma': [1e-4, 1e-3, 1e-2, 1e-1],\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'degree': [2, 3]\n",
        "    }\n",
        "]\n",
        "optimalPolySVM = GridSearchCV(svm.SVC(decision_function_shape = 'ovr'),\n",
        "                              parameters, cv = 3, n_jobs = -1, verbose = 0)\n",
        "optimalPolySVM.fit(trainSentencesVectors, trainLabels)\n",
        "        \n",
        "# Comparacion\n",
        "\n",
        "svmsWithW2V = {}\n",
        "\n",
        "svmsWithW2V['SVM Lineal con CountVectorizer'] = \\\n",
        "    svm.SVC(kernel = optimalLinearSVM.best_params_['kernel'],\n",
        "            C = optimalLinearSVM.best_params_['C'])\n",
        "        \n",
        "svmsWithW2V['SVM Radial con CountVectorizer'] = \\\n",
        "    svm.SVC(kernel = optimalRadialSVM.best_params_['kernel'],\n",
        "            C = optimalRadialSVM.best_params_['C'],\n",
        "            gamma = optimalRadialSVM.best_params_['gamma'])\n",
        "        \n",
        "svmsWithW2V['SVM Polinómico con CountVectorizer'] = \\\n",
        "    svm.SVC(kernel = optimalPolySVM.best_params_['kernel'],\n",
        "            C = optimalPolySVM.best_params_['C'],\n",
        "            gamma = optimalPolySVM.best_params_['gamma'],\n",
        "            degree = optimalPolySVM.best_params_['degree'])\n",
        "    \n",
        "    \n",
        "results = compareSVMClassifiers(svmsWithW2V,\n",
        "                                trainSentencesVectors, trainLabels,\n",
        "                                testSentencesVectors, testLabels)\n",
        "\n",
        "comparisonResults = comparisonResults.append(results)\n",
        "        \n",
        "    \n",
        "# Redes neuronales convolucionales\n",
        "\n",
        "print(('\\t***************************************************************\\n'\n",
        "       '\\tRNC\\n'\n",
        "       '\\t***************************************************************\\n'\n",
        "       '\\n'))\n",
        "    \n",
        "# Redimensionando los vectores para ser alimentados a la RNC\n",
        "trainVectors = trainSentencesVectors.reshape((trainSentencesVectors.shape[0], 1,\n",
        "                                     trainSentencesVectors.shape[1]))\n",
        "testVectors = testSentencesVectors.reshape((testSentencesVectors.shape[0], 1,\n",
        "                                   testSentencesVectors.shape[1]))\n",
        "    \n",
        "# Conversión de las etiquetas (One-Hot)\n",
        "trainLabelsCat = keras.utils.np_utils.to_categorical(trainLabels, 3)\n",
        "testLabelsCat = keras.utils.np_utils.to_categorical(testLabels, 3)\n",
        "    \n",
        "def buildModel(inputDimension):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv1D(128, 1, input_shape = (1, inputDimension),\n",
        "              activation = 'relu'))\n",
        "    model.add(Conv1D(32, 1, activation = 'relu'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(64, activation = 'relu'))\n",
        "    model.add(Dense(32, activation = 'relu'))    \n",
        "    model.add(Dense(16, activation = 'relu'))\n",
        "    model.add(Dense(3, activation = 'sigmoid'))\n",
        "\n",
        "    model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy',\n",
        "                  metrics = ['accuracy'])\n",
        "\n",
        "    return model\n",
        "    \n",
        "modelFileName = 'modelo.hdf5'\n",
        "    \n",
        "# Checkpoint \n",
        "checkpointer = ModelCheckpoint(filepath = modelFileName, \n",
        "                               monitor = 'val_acc', verbose = 0,\n",
        "                               save_best_only = True, mode = 'auto')\n",
        "\n",
        "# Reducción de taza de aprendizaje \n",
        "learningRateReducer = ReduceLROnPlateau(monitor = 'val_acc', factor = 0.2,\n",
        "                                        patience = 5, min_lr = 0.001,\n",
        "                                        verbose = 0)\n",
        "\n",
        "# Detener el entrenamiento al alcanzar el \"óptimo\"\n",
        "earlyStop = EarlyStopping(monitor = 'val_acc', patience = 50, verbose = 0,\n",
        "                          mode = 'auto')\n",
        "    \n",
        "model = buildModel(trainVectors.shape[2])\n",
        "model.fit(trainVectors, trainLabelsCat, batch_size = 32,\n",
        "          validation_data = (testVectors, testLabelsCat), epochs = 1000,\n",
        "          callbacks = [checkpointer, learningRateReducer, earlyStop],\n",
        "          verbose = 0, shuffle = False)\n",
        "    \n",
        "# Contrucción del modelo\n",
        "model = buildModel(trainVectors.shape[2])\n",
        "\n",
        "# Carga delos pesos\n",
        "model.load_weights(modelFileName)\n",
        "acc = model.evaluate(trainVectors, trainLabelsCat, verbose = 0)[1]\n",
        "valacc = model.evaluate(testVectors, testLabelsCat, verbose = 0)[1]\n",
        "culumnNames = ['Clasificador', 'Exactitud Entrenamiento',\n",
        "               'Exactitud Prueba']\n",
        "results = pd.DataFrame(data = np.zeros(shape = (1, 3)),\n",
        "                       columns = culumnNames)\n",
        "results.loc[0, 'Clasificador'] = 'RNC con CountVectorizer'\n",
        "results.loc[0, 'Exactitud Entrenamiento'] = acc\n",
        "results.loc[0, 'Exactitud Prueba'] = valacc\n",
        "    \n",
        "comparisonResults = comparisonResults.append(results)\n",
        "\n",
        "    \n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "## MOSTRANDO LOS RESULTADOS ####################################################\n",
        "################################################################################\n",
        "\n",
        "display(comparisonResults.sort_values(by = 'Exactitud Prueba',\n",
        "                                      ascending = False))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "\n",
            "*****************************************************************\n",
            "INICIANDO PRUEBAS CON WORD2VEC\n",
            "*****************************************************************\n",
            "\n",
            "\n",
            "\t***************************************************************\n",
            "\tW2V 200\n",
            "\t***************************************************************\n",
            "\t\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 480/480 [00:00<00:00, 814427.96it/s]\n",
            "100%|██████████| 480/480 [00:00<00:00, 148976.31it/s]\n",
            "  0%|          | 0/480 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "100%|██████████| 480/480 [00:00<00:00, 4481.73it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 4481.89it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t\t*************************************************************\n",
            "\t\tSVM Lineal\n",
            "\t\t*************************************************************\n",
            "\n",
            "\n",
            "\t\t*************************************************************\n",
            "\t\tSVM Radial\n",
            "\t\t*************************************************************\n",
            "\n",
            "\n",
            "\t\t*************************************************************\n",
            "\t\tSVM Polinómico\n",
            "\t\t*************************************************************\n",
            "\n",
            "\n",
            "\t\t*************************************************************\n",
            "\t\tRNC\n",
            "\t\t*************************************************************\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v3HbdiddXOjc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}